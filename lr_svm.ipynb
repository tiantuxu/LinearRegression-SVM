{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataFilename = 'trainingSet.csv'\n",
    "testDataFilename = 'testSet.csv'\n",
    "modelIdx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(scores):\n",
    "    predictions = np.zeros(len(scores))\n",
    "    for i in range(len(predictions)):\n",
    "        if scores[i] >= 0:\n",
    "            predictions[i] +=  1.0 / (1.0 + np.exp(-scores[i]))\n",
    "        else:\n",
    "            predictions[i] += np.exp(scores[i]) / (1.0 + np.exp(scores[i]))\n",
    "    return predictions\n",
    "\n",
    "def lr(trainingSet, testSet):\n",
    "    print len(trainingSet.columns)\n",
    "    regularization = 0.01\n",
    "    step_size = 0.01\n",
    "    \n",
    "    max_iterations = 500\n",
    "    tol = 1e-6\n",
    "    \n",
    "    count = 0\n",
    "    diff = 1.0\n",
    "    \n",
    "    train_labels = trainingSet['decision']\n",
    "    test_labels = testSet['decision']\n",
    "    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    testSet = testSet.drop('decision', axis=1)\n",
    "    \n",
    "    #print train_labels, trainingSet\n",
    "    w = np.zeros(len(trainingSet.columns) + 1)\n",
    "    \n",
    "    # Add intercept\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    #X = np.concatenate((X, intercept.T), axis=1)\n",
    "    X = np.hstack((X, intercept))\n",
    "    \n",
    "    while(count < max_iterations and diff > tol):\n",
    "        count += 1\n",
    "        norm_old = np.linalg.norm(w)\n",
    "        \n",
    "        scores = np.dot(X, w)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        gradient = np.dot(X.T, (predictions - Y))\n",
    "\n",
    "        for j in range(len(w)):\n",
    "            gradient[j] += regularization * w[j]\n",
    "            \n",
    "        #gradient /= len(train_labels)\n",
    "        w -= step_size * gradient\n",
    "            \n",
    "        norm_new = np.linalg.norm(w)\n",
    "        \n",
    "        diff = abs(norm_new - norm_old)\n",
    "        #print w\n",
    "        print count, diff\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(trainingSet, testSet):\n",
    "    print len(trainingSet.columns)\n",
    "    regularization = 0.01\n",
    "    step_size = 0.5\n",
    "    \n",
    "    max_iterations = 500\n",
    "    tol = 1e-6\n",
    "    \n",
    "    count = 0\n",
    "    diff = 1.0\n",
    "    \n",
    "    train_labels = trainingSet['decision']\n",
    "    test_labels = testSet['decision']\n",
    "    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    testSet = testSet.drop('decision', axis=1)\n",
    "    \n",
    "    w = np.zeros(len(trainingSet.columns) + 1)\n",
    "    \n",
    "    # Add intercept\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "        if Y[i] == 0:\n",
    "            Y[i] = -1.0\n",
    "        else:\n",
    "            Y[i] = 1.0\n",
    "    \n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((X, intercept))\n",
    "    \n",
    "    while(count < max_iterations and diff > tol):\n",
    "        count += 1\n",
    "        norm_old = np.linalg.norm(w)\n",
    "        \n",
    "        predictions = np.dot(X, w)\n",
    "\n",
    "        gradient = np.zeros(X.shape[1])\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] * Y[i] < 1.0:\n",
    "                gradient -= Y[i] * X[i]\n",
    "            \n",
    "        gradient /= len(train_labels)\n",
    "        #print gradient.shape\n",
    "        \n",
    "        for j in range(len(w) - 1):\n",
    "            gradient[j] += 1.0 * regularization * w[j]\n",
    "\n",
    "        #gradient /= len(train_labels)\n",
    "        w -= step_size * gradient\n",
    "        norm_new = np.linalg.norm(w)\n",
    "        \n",
    "        diff = abs(norm_new - norm_old)\n",
    "        print count, diff\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n",
      "1 3.3390134775333355\n",
      "2 5.102754375694117\n",
      "3 1.7424639012953653\n",
      "4 1.082706474824259\n",
      "5 4.40950742290708\n",
      "6 6.251203971240096\n",
      "7 9.54483690793526\n",
      "8 8.99377302443336\n",
      "9 6.952230114911771\n",
      "10 4.536711086625102\n",
      "11 2.206644962245921\n",
      "12 0.7626290062937056\n",
      "13 2.256147557604649\n",
      "14 5.3751560845740745\n",
      "15 5.546985262636933\n",
      "16 6.768573716125725\n",
      "17 6.213986063382415\n",
      "18 1.474639274023926\n",
      "19 3.277340751504049\n",
      "20 3.7568138378250318\n",
      "21 6.719150773238768\n",
      "22 6.0475515157692445\n",
      "23 4.0800176320092625\n",
      "24 2.1444537006715425\n",
      "25 0.8416070412852203\n",
      "26 1.6013577850836427\n",
      "27 2.0779656339494075\n",
      "28 4.7761526756133605\n",
      "29 4.363502332903405\n",
      "30 0.2056918274983488\n",
      "31 3.2713164940234734\n",
      "32 1.495677706804031\n",
      "33 0.5396242570613481\n",
      "34 1.7099495347727647\n",
      "35 1.9301067780512025\n",
      "36 4.366583501566483\n",
      "37 3.8729286268549306\n",
      "38 0.2731474026424614\n",
      "39 0.6699386629508677\n",
      "40 0.7916263378592898\n",
      "41 1.28968268725985\n",
      "42 1.502788870869642\n",
      "43 3.8180439820245358\n",
      "44 3.3778275976779497\n",
      "45 0.04005166743330335\n",
      "46 2.2626640655676056\n",
      "47 0.6377740118002926\n",
      "48 0.1013634869233293\n",
      "49 1.9346438011200675\n",
      "50 1.8647053623274488\n",
      "51 3.4011145740194557\n",
      "52 2.9560344640631264\n",
      "53 0.017856838138079922\n",
      "54 1.7479544641471172\n",
      "55 0.15905761991963985\n",
      "56 0.17753449608090577\n",
      "57 2.1166033868422645\n",
      "58 1.9138546190461625\n",
      "59 2.5543824111128295\n",
      "60 2.186870753175292\n",
      "61 1.6206372510012912\n",
      "62 1.2038104520808943\n",
      "63 2.8432172259120243\n",
      "64 2.434919559696901\n",
      "65 0.3409292204128853\n",
      "66 0.6701390961531004\n",
      "67 0.9985186105664106\n",
      "68 0.9834303897221872\n",
      "69 2.5759723368922387\n",
      "70 2.1908517558117353\n",
      "71 0.49640021037255266\n",
      "72 0.2707895439594772\n",
      "73 1.3980440060915242\n",
      "74 1.2552511524462098\n",
      "75 2.281931937963485\n",
      "76 1.9286325755800284\n",
      "77 0.8089777904006645\n",
      "78 0.29362103560161756\n",
      "79 1.92844518259189\n",
      "80 1.6392943597252874\n",
      "81 1.3610575453043694\n",
      "82 1.0216274447402292\n",
      "83 2.1121134313402266\n",
      "84 1.7720528236324142\n",
      "85 0.781504612931041\n",
      "86 0.31823557489614274\n",
      "87 1.8493355346428935\n",
      "88 1.5520500852231756\n",
      "89 1.110252281676189\n",
      "90 0.7653043134902653\n",
      "91 1.9328975425868968\n",
      "92 1.6100464013832507\n",
      "93 0.8029367306364463\n",
      "94 0.4000238499926212\n",
      "95 1.776548354056139\n",
      "96 1.4758428301493147\n",
      "97 0.9141415354527425\n",
      "98 0.5639674799478058\n",
      "99 1.7738478873840648\n",
      "100 1.4673154329234848\n",
      "101 0.7861711815421089\n",
      "102 0.4209981963128584\n",
      "103 1.6709815390091336\n",
      "104 1.377674428728696\n",
      "105 0.8368241912623198\n",
      "106 0.5031975876332986\n",
      "107 1.6262592044539943\n",
      "108 1.3366521408716885\n",
      "109 0.7955989498720655\n",
      "110 0.46810000332463275\n",
      "111 1.5619862374429196\n",
      "112 1.2799497083933886\n",
      "113 0.7917193685853512\n",
      "114 0.47841427898548616\n",
      "115 1.5036704797048799\n",
      "116 1.228712603538849\n",
      "117 0.7924551538486853\n",
      "118 0.49299492417655344\n",
      "119 1.4626626616873892\n",
      "120 1.1923672377909291\n",
      "121 0.7639761521475208\n",
      "122 0.47140834252356356\n",
      "123 1.4088667716382162\n",
      "124 1.1454764523167817\n",
      "125 0.7615521188417098\n",
      "126 0.4804777882168736\n",
      "127 1.3663882896959763\n",
      "128 1.1084540017004656\n",
      "129 0.7459911753209525\n",
      "130 0.4731574156422411\n",
      "131 1.3191298777358043\n",
      "132 1.0675628504574846\n",
      "133 0.7658183274245189\n",
      "134 0.506644453527116\n",
      "135 1.265953211177468\n",
      "136 1.0216561197669591\n",
      "137 0.7801952691885035\n",
      "138 0.5323330151665928\n",
      "139 1.2229357451234009\n",
      "140 0.9846457360084244\n",
      "141 0.7773778303269054\n",
      "142 0.5374904760017856\n",
      "143 1.1772887324708492\n",
      "144 0.9452535667569038\n",
      "145 0.7772174878182554\n",
      "146 0.5450301604143561\n",
      "147 1.11854112020891\n",
      "148 0.8939464081206339\n",
      "149 0.8020304983186506\n",
      "150 0.5792802860330255\n",
      "151 1.0595245842985648\n",
      "152 0.8420528951490596\n",
      "153 0.8378566488098258\n",
      "154 0.6237484750159368\n",
      "155 0.9998008965695817\n",
      "156 0.788659173762035\n",
      "157 0.8556316152086509\n",
      "158 0.6476575276549568\n",
      "159 0.9413167969047507\n",
      "160 0.7357275261819112\n",
      "161 0.8620417297851617\n",
      "162 0.6589148178929349\n",
      "163 0.9033778241395609\n",
      "164 0.7021849289356794\n",
      "165 0.8508317939456802\n",
      "166 0.6518085303376395\n",
      "167 0.8719188487119922\n",
      "168 0.6748199927496827\n",
      "169 0.8388774476209022\n",
      "170 0.6438158075053906\n",
      "171 0.8471942842648161\n",
      "172 0.6540123257252333\n",
      "173 0.8191738713653791\n",
      "174 0.6279412063873551\n",
      "175 0.827680306787606\n",
      "176 0.6382958898855904\n",
      "177 0.8001917665487284\n",
      "178 0.6127232099726925\n",
      "179 0.8074488661358998\n",
      "180 0.621798874606057\n",
      "181 0.7819385038095632\n",
      "182 0.5981590135496049\n",
      "183 0.7864793464660025\n",
      "184 0.6045006557332755\n",
      "185 0.7689775956735758\n",
      "186 0.5888274613248186\n",
      "187 0.7703996960235173\n",
      "188 0.5920050327674318\n",
      "189 0.7487088170349026\n",
      "190 0.5721008276793356\n",
      "191 0.7520766735047175\n",
      "192 0.5771982698813218\n",
      "193 0.736570495980807\n",
      "194 0.5634524761451729\n",
      "195 0.7358578524246937\n",
      "196 0.5644323556473552\n",
      "197 0.71762564778561\n",
      "198 0.547914094754681\n",
      "199 0.7196856372321676\n",
      "200 0.5516365813144901\n",
      "201 0.7034543599869494\n",
      "202 0.5370967413361143\n",
      "203 0.703185871996034\n",
      "204 0.5384588179949716\n",
      "205 0.6887001139797277\n",
      "206 0.5256232024481875\n",
      "207 0.6886251414860709\n",
      "208 0.5271476292620747\n",
      "209 0.6744369519166256\n",
      "210 0.5145790403785071\n",
      "211 0.6745484331820535\n",
      "212 0.5162571397136659\n",
      "213 0.659272880791427\n",
      "214 0.5025704005911251\n",
      "215 0.6595770208694844\n",
      "216 0.5044129161907875\n",
      "217 0.6472783089715364\n",
      "218 0.4936688673876759\n",
      "219 0.6477348720247136\n",
      "220 0.49563086398878653\n",
      "221 0.6304222359036586\n",
      "222 0.4798461863581167\n",
      "223 0.6338383525556779\n",
      "224 0.4847351688476067\n",
      "225 0.620681893940521\n",
      "226 0.4730816502650512\n",
      "227 0.6214680159290964\n",
      "228 0.4753146422692538\n",
      "229 0.6060008648263207\n",
      "230 0.4613155530501416\n",
      "231 0.6081032367525907\n",
      "232 0.4648377676695006\n",
      "233 0.5980708847388314\n",
      "234 0.4562442116819696\n",
      "235 0.59364692258697\n",
      "236 0.45322556658210544\n",
      "237 0.5879060790900965\n",
      "238 0.4488818691009868\n",
      "239 0.582256687226625\n",
      "240 0.44461331756631495\n",
      "241 0.576694387552763\n",
      "242 0.4404182468569928\n",
      "243 0.5712174312053264\n",
      "244 0.4362950383993436\n",
      "245 0.5658241153762873\n",
      "246 0.4322421185856484\n",
      "247 0.5605127817928306\n",
      "248 0.428257957255866\n",
      "249 0.5552818152574304\n",
      "250 0.4243410662396627\n",
      "251 0.5501296422448334\n",
      "252 0.42177194890962966\n",
      "253 0.5450208747070064\n",
      "254 0.4192640568182142\n",
      "255 0.536304337399983\n",
      "256 0.41564473459752804\n",
      "257 0.5315531327448895\n",
      "258 0.41734992913034574\n",
      "259 0.5229564363926258\n",
      "260 0.4098953283123947\n",
      "261 0.5208413185240914\n",
      "262 0.40889388238826285\n",
      "263 0.5135179190882226\n",
      "264 0.4026840075216356\n",
      "265 0.5141084988480387\n",
      "266 0.4043686579232286\n",
      "267 0.5058012953472684\n",
      "268 0.3984238888390692\n",
      "269 0.5051643442608693\n",
      "270 0.3988436358682321\n",
      "271 0.4982561055522936\n",
      "272 0.3943205607605407\n",
      "273 0.49397160280128816\n",
      "274 0.3923899886702529\n",
      "275 0.4898635534363365\n",
      "276 0.3892905758428\n",
      "277 0.48825530103744796\n",
      "278 0.3886741876513824\n",
      "279 0.4842270341361399\n",
      "280 0.38563784727989514\n",
      "281 0.4802590370980795\n",
      "282 0.3826468607034883\n",
      "283 0.4763451434684356\n",
      "284 0.3809525979154884\n",
      "285 0.4737299453453758\n",
      "286 0.3818197367114422\n",
      "287 0.4698183290515061\n",
      "288 0.38018340685886187\n",
      "289 0.4659898777618423\n",
      "290 0.3772442815471777\n",
      "291 0.46353948388278354\n",
      "292 0.37567021676235157\n",
      "293 0.45979651350648965\n",
      "294 0.3727991532091579\n",
      "295 0.4574250585130315\n",
      "296 0.3712870399258321\n",
      "297 0.45385735736187627\n",
      "298 0.3685744980004628\n",
      "299 0.45156143151405104\n",
      "300 0.36712105353447555\n",
      "301 0.44807214225497205\n",
      "302 0.36447052428874827\n",
      "303 0.4446307890916472\n",
      "304 0.3618573511953471\n",
      "305 0.44123427949117655\n",
      "306 0.36051557651681065\n",
      "307 0.4391116920157998\n",
      "308 0.35919215725549947\n",
      "309 0.4357957504189116\n",
      "310 0.35666969921420844\n",
      "311 0.4337427203373849\n",
      "312 0.35539838607522967\n",
      "313 0.4304974013910652\n",
      "314 0.3529311525376926\n",
      "315 0.42970936490845446\n",
      "316 0.35290697088093737\n",
      "317 0.4241720112828702\n",
      "318 0.34942369022218145\n",
      "319 0.42474137469090323\n",
      "320 0.35072390267561815\n",
      "321 0.4168207313778183\n",
      "322 0.34354952287066\n",
      "323 0.41861807763818604\n",
      "324 0.3460598551027658\n",
      "325 0.41214146173413013\n",
      "326 0.34031380708525916\n",
      "327 0.4139838509433673\n",
      "328 0.34285629494644354\n",
      "329 0.40757678356587945\n",
      "330 0.33716617441782404\n",
      "331 0.40946457183130036\n",
      "332 0.3397396825081742\n",
      "333 0.4019447133992031\n",
      "334 0.3365300712277133\n",
      "335 0.405073946094646\n",
      "336 0.3402908586113398\n",
      "337 0.3998653074397396\n",
      "338 0.3357442176098502\n",
      "339 0.39474523741772316\n",
      "340 0.3349327572366221\n",
      "341 0.39335209436358554\n",
      "342 0.3353819093060082\n",
      "343 0.39322317763966197\n",
      "344 0.335824795287472\n",
      "345 0.39165797578859696\n",
      "346 0.3348290864556702\n",
      "347 0.3888618411309892\n",
      "348 0.3350649444792353\n",
      "349 0.38487816549314857\n",
      "350 0.33161846085340585\n",
      "351 0.3858753043734211\n",
      "352 0.3331337858334109\n",
      "353 0.38192987763610375\n",
      "354 0.3309065324863951\n",
      "355 0.3805191487786814\n",
      "356 0.33113232715567875\n",
      "357 0.3789622555871617\n",
      "358 0.33006439821770783\n",
      "359 0.378715793502046\n",
      "360 0.33029816350735075\n",
      "361 0.3771804992064176\n",
      "362 0.3292427697383289\n",
      "363 0.37566531849110874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364 0.3292735146542043\n",
      "365 0.37292730819359576\n",
      "366 0.3269929364455919\n",
      "367 0.37381439702070196\n",
      "368 0.32833047326536047\n",
      "369 0.36867480805107533\n",
      "370 0.32482486499519325\n",
      "371 0.37078553477081755\n",
      "372 0.3261830277371729\n",
      "373 0.36810605103075744\n",
      "374 0.3251257694460108\n",
      "375 0.36663593587982035\n",
      "376 0.3240796286374348\n",
      "377 0.3651826928071742\n",
      "378 0.32423918749358904\n",
      "379 0.3636736337672204\n",
      "380 0.3243209809267924\n",
      "381 0.3610348903944498\n",
      "382 0.323330676589066\n",
      "383 0.35968365044885786\n",
      "384 0.32346278882915414\n",
      "385 0.3594571432119693\n",
      "386 0.32359167815771883\n",
      "387 0.358012010715953\n",
      "388 0.32250097197457706\n",
      "389 0.356585322752899\n",
      "390 0.32142309134873415\n",
      "391 0.3551746664250004\n",
      "392 0.3203578687365223\n",
      "393 0.3561410433687229\n",
      "394 0.32166188509864213\n",
      "395 0.3535027432294129\n",
      "396 0.31936920096702437\n",
      "397 0.35327061443307883\n",
      "398 0.3206285989927622\n",
      "399 0.35182608539701477\n",
      "400 0.31950913830890215\n",
      "401 0.35040237781954175\n",
      "402 0.31969938733154635\n",
      "403 0.3502886724584684\n",
      "404 0.3198864568035873\n",
      "405 0.3501761050251275\n",
      "406 0.32007176883997346\n",
      "407 0.35006466687705284\n",
      "408 0.3202553412701903\n",
      "409 0.3487938857306929\n",
      "410 0.3192788207298207\n",
      "411 0.34869872758254417\n",
      "412 0.3194712035408287\n",
      "413 0.3452539747486796\n",
      "414 0.3175045789664992\n",
      "415 0.3463821723250078\n",
      "416 0.3188989763270911\n",
      "417 0.3420332933209309\n",
      "418 0.3148276112269315\n",
      "419 0.3455852561265189\n",
      "420 0.3186328081629455\n",
      "421 0.34124261218409657\n",
      "422 0.3145692373309146\n",
      "423 0.3435430096363703\n",
      "424 0.31711953331760867\n",
      "425 0.3392297928043817\n",
      "426 0.31307663724651746\n",
      "427 0.34393538745954544\n",
      "428 0.3180212567410763\n",
      "429 0.33961324438916307\n",
      "430 0.3139737560510625\n",
      "431 0.339707440274033\n",
      "432 0.3143109613895092\n",
      "433 0.33874413638415035\n",
      "434 0.3135991567881291\n",
      "435 0.3389189057887947\n",
      "436 0.3140188481554276\n",
      "437 0.3368799792795443\n",
      "438 0.3122284560908497\n",
      "439 0.3370754642154594\n",
      "440 0.31266261078707913\n",
      "441 0.33726539508403164\n",
      "442 0.31309227280409857\n",
      "443 0.3330414119932854\n",
      "444 0.30911187748637303\n",
      "445 0.3366981176460868\n",
      "446 0.31299161836287226\n",
      "447 0.33247681165388343\n",
      "448 0.3101257728715012\n",
      "449 0.3350754615100513\n",
      "450 0.31182620695742\n",
      "451 0.3319996247377759\n",
      "452 0.3100953061984555\n",
      "453 0.334600106438387\n",
      "454 0.31290178720055195\n",
      "455 0.331515576945705\n",
      "456 0.31004225040577893\n",
      "457 0.33074157052516995\n",
      "458 0.3094747783237608\n",
      "459 0.33227009316921396\n",
      "460 0.3112077967591418\n",
      "461 0.32921723180926676\n",
      "462 0.30837008697122315\n",
      "463 0.33184264353123893\n",
      "464 0.3111897793643692\n",
      "465 0.327770894650655\n",
      "466 0.3073331271771309\n",
      "467 0.33040998866117377\n",
      "468 0.31016177242329945\n",
      "469 0.32747238246368937\n",
      "470 0.3074338638002132\n",
      "471 0.32903481851174377\n",
      "472 0.30918519300126945\n",
      "473 0.32611612791897926\n",
      "474 0.30646994756022394\n",
      "475 0.32876761186295766\n",
      "476 0.3093039653216678\n",
      "477 0.32477060390761636\n",
      "478 0.30663665453847955\n",
      "479 0.32643439245695305\n",
      "480 0.3073457147601246\n",
      "481 0.3235496602695136\n",
      "482 0.3057800016809651\n",
      "483 0.3252506144145144\n",
      "484 0.3076494811491912\n",
      "485 0.3224232951515731\n",
      "486 0.30500344602909735\n",
      "487 0.3241363189515525\n",
      "488 0.3068800564516252\n",
      "489 0.32132279922692675\n",
      "490 0.3042441225638086\n",
      "491 0.32411893428115945\n",
      "492 0.30719786168898366\n",
      "493 0.32014133847149395\n",
      "494 0.3033989022024386\n",
      "495 0.3229490218581432\n",
      "496 0.3063598071385556\n",
      "497 0.3189867862576392\n",
      "498 0.3025723004026375\n",
      "499 0.3218040325708529\n",
      "500 0.3055402029376637\n"
     ]
    }
   ],
   "source": [
    "trainingSet = pd.read_csv(trainingDataFilename)\n",
    "testSet = pd.read_csv(testDataFilename)\n",
    "\n",
    "if modelIdx == 1:\n",
    "    w = lr(trainingSet, testSet)\n",
    "else:\n",
    "    w = svm(trainingSet, testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_lr(w, trainingSet, testSet):\n",
    "    total_train = len(trainingSet)\n",
    "    count_train = 0\n",
    "    total_test = len(testSet)\n",
    "    count_test = 0\n",
    "    \n",
    "    train_labels = trainingSet['decision']\n",
    "    test_labels = testSet['decision']\n",
    "    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    testSet = testSet.drop('decision', axis=1)\n",
    "    \n",
    "    # Training accuracy\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((X, intercept))\n",
    "    \n",
    "    scores = np.dot(X, w)\n",
    "    predictions = sigmoid(scores)\n",
    "        \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.5:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(train_labels[i]):\n",
    "            count_train += 1\n",
    "\n",
    "    training_accuracy = 1.0 * count_train/total_train\n",
    "    print 'Training Accuracy LR:', '%.2f' % training_accuracy\n",
    "    \n",
    "    # Test accuracy\n",
    "    X = np.array(testSet)\n",
    "    Y = np.array(test_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((X, intercept))\n",
    "\n",
    "    scores = np.dot(X, w)\n",
    "    predictions = sigmoid(scores)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.5:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(test_labels[i]):\n",
    "            count_test += 1\n",
    "            \n",
    "    test_accuracy = 1.0 * count_test/total_test\n",
    "    print 'Test Accuracy LR:', '%.2f' % test_accuracy\n",
    "    \n",
    "def get_accuracy_svm(w, trainingSet, testSet):\n",
    "    total_train = len(trainingSet)\n",
    "    count_train = 0\n",
    "    total_test = len(testSet)\n",
    "    count_test = 0\n",
    "    \n",
    "    train_labels = trainingSet['decision']\n",
    "    test_labels = testSet['decision']\n",
    "    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    testSet = testSet.drop('decision', axis=1)\n",
    "    \n",
    "    # Training accuracy\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    print Y\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((X, intercept))\n",
    "    \n",
    "    predictions = np.dot(X, w)\n",
    "        \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] >= 0.0:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(train_labels[i]):\n",
    "            count_train += 1\n",
    "\n",
    "    training_accuracy = 1.0 * count_train/total_train\n",
    "    print 'Training Accuracy SVM:', '%.2f' % training_accuracy\n",
    "    \n",
    "    # Test accuracy\n",
    "    X = np.array(testSet)\n",
    "    Y = np.array(test_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((X, intercept))\n",
    "\n",
    "    predictions = np.dot(X, w)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] >= 0.0:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(test_labels[i]):\n",
    "            count_test += 1\n",
    "            \n",
    "    test_accuracy = 1.0 * count_test/total_test\n",
    "    print 'Test Accuracy SVM:', '%.2f' % test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 0 0]\n",
      "Training Accuracy SVM: 0.56\n",
      "Test Accuracy SVM: 0.55\n"
     ]
    }
   ],
   "source": [
    "if modelIdx == 1:\n",
    "    get_accuracy_lr(w, trainingSet, testSet)\n",
    "else:\n",
    "    get_accuracy_svm(w, trainingSet, testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
