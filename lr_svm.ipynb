{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataFilename = 'trainingSet.csv'\n",
    "testDataFilename = 'testSet.csv'\n",
    "modelIdx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(scores):\n",
    "    predictions = np.zeros(len(scores))\n",
    "    for i in range(len(predictions)):\n",
    "        if scores[i] >= 0:\n",
    "            predictions[i] +=  1.0 / (1.0 + np.exp(-scores[i]))\n",
    "        else:\n",
    "            predictions[i] += np.exp(scores[i]) / (1.0 + np.exp(scores[i]))\n",
    "    return predictions\n",
    "\n",
    "def lr(trainingSet, testSet):\n",
    "    print len(trainingSet.columns)\n",
    "    regularization = 0.01\n",
    "    step_size = 0.01\n",
    "    \n",
    "    max_iterations = 500\n",
    "    tol = 1e-6\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    train_labels = trainingSet['decision']    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    \n",
    "    #print train_labels, trainingSet\n",
    "    w = np.zeros(len(trainingSet.columns) + 1)\n",
    "    \n",
    "    # Add intercept\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    #X = np.concatenate((X, intercept.T), axis=1)\n",
    "    X = np.hstack((X, intercept))\n",
    "    diff = 100.0\n",
    "    \n",
    "    while(count < max_iterations and diff > tol):\n",
    "        count += 1\n",
    "        norm_old = np.linalg.norm(w)\n",
    "        \n",
    "        scores = np.dot(X, w)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        gradient = np.dot(X.T, (predictions - Y))\n",
    "\n",
    "        for j in range(len(w)):\n",
    "            gradient[j] += regularization * w[j]\n",
    "            \n",
    "        #gradient /= len(train_labels)\n",
    "        w -= step_size * gradient\n",
    "        norm_new = np.linalg.norm(w)\n",
    "        \n",
    "        diff = abs(norm_new - norm_old)\n",
    "        #print w\n",
    "        print count, diff\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(trainingSet, testSet):\n",
    "    #print len(trainingSet.columns)\n",
    "    regularization = 0.01\n",
    "    step_size = 0.50\n",
    "    \n",
    "    max_iterations = 500\n",
    "    tol = 1e-6\n",
    "    #print len(trainingSet[trainingSet['decision'] == 1])\n",
    "    count = 0\n",
    "    train_labels = trainingSet['decision']    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "\n",
    "    w = np.zeros(len(trainingSet.columns) + 1)\n",
    "    \n",
    "    # Add intercept\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    #print train_labels\n",
    "    for i in range(len(Y)):\n",
    "        if Y[i] == 0:\n",
    "            Y[i] = -1.0\n",
    "        else:\n",
    "            Y[i] = 1.0\n",
    "    #print Y.tolist()\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((intercept, X))\n",
    "    diff = 100.0\n",
    "    while(count < max_iterations and diff > tol):\n",
    "        count += 1\n",
    "        norm_old = np.linalg.norm(w)\n",
    "        \n",
    "        predictions = np.dot(X, w)\n",
    "        #print X.shape, w.shape,predictions.shape\n",
    "#         p = []\n",
    "#         for i in range(0, 5200):\n",
    "#             p.append(sum([w[j] * X[i][j] for j in range(len(trainingSet.columns))]))\n",
    "        \n",
    "#         print predictions, p \n",
    "        #print len(predictions)\n",
    "        #print np.dot(w, X[0]), predictions[0]     \n",
    "        error = 0\n",
    "        gradient = np.zeros(len(w))\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] * Y[i] < 1.0:\n",
    "                error += 1\n",
    "                #gradient -= 1.0 * Y[i] * X[i]\n",
    "                gradient -= np.multiply(X[i], Y[i])\n",
    "            \n",
    "        gradient /= 1.0 * len(train_labels)\n",
    "        #print gradient.shape, X[0].shape\n",
    "        \n",
    "        for j in range(1, len(gradient)):\n",
    "            gradient[j] += 1.0 * regularization * w[j]\n",
    "\n",
    "        w -= 1.0 * step_size * gradient\n",
    "        norm_new = np.linalg.norm(w)\n",
    "        diff = abs(norm_new - norm_old)\n",
    "        print count, diff, error\n",
    "    #print w\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_lr(w, trainingSet, testSet):\n",
    "    total_train = len(trainingSet)\n",
    "    count_train = 0\n",
    "    total_test = len(testSet)\n",
    "    count_test = 0\n",
    "    \n",
    "    train_labels = trainingSet['decision']\n",
    "    test_labels = testSet['decision']\n",
    "    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    testSet = testSet.drop('decision', axis=1)\n",
    "    \n",
    "    # Training accuracy\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((X, intercept))\n",
    "    \n",
    "    scores = np.dot(X, w)\n",
    "    predictions = sigmoid(scores)\n",
    "        \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.5:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(train_labels[i]):\n",
    "            count_train += 1\n",
    "\n",
    "    training_accuracy = 1.0 * count_train/total_train\n",
    "    print 'Training Accuracy LR:', '%.2f' % training_accuracy, count_train, total_train\n",
    "    \n",
    "    # Test accuracy\n",
    "    X = np.array(testSet)\n",
    "    Y = np.array(test_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((X, intercept))\n",
    "\n",
    "    scores = np.dot(X, w)\n",
    "    predictions = sigmoid(scores)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.5:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(test_labels[i]):\n",
    "            count_test += 1\n",
    "            \n",
    "    test_accuracy = 1.0 * count_test/total_test\n",
    "    print 'Test Accuracy LR:', '%.2f' % test_accuracy, count_test, total_test\n",
    "    \n",
    "def get_accuracy_svm(w, trainingSet, testSet):\n",
    "    total_train = len(trainingSet)\n",
    "    count_train = 0\n",
    "    total_test = len(testSet)\n",
    "    count_test = 0\n",
    "    \n",
    "    train_labels = trainingSet['decision']\n",
    "    test_labels = testSet['decision']\n",
    "    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    testSet = testSet.drop('decision', axis=1)\n",
    "    \n",
    "    # Training accuracy\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((intercept, X))\n",
    "    \n",
    "    predictions = np.dot(X, w)\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.0:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(Y[i]):\n",
    "            count_train += 1\n",
    "\n",
    "    training_accuracy = 1.0 * count_train/total_train\n",
    "    print 'Training Accuracy SVM:', '%.2f' % training_accuracy\n",
    "    \n",
    "    # Test accuracy\n",
    "    X = np.array(testSet)\n",
    "    Y = np.array(test_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((intercept, X))\n",
    "\n",
    "    predictions = np.dot(X, w)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.0:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(Y[i]):\n",
    "            count_test += 1\n",
    "            \n",
    "    test_accuracy = 1.0 * count_test/total_test\n",
    "    print 'Test Accuracy SVM:', '%.2f' % test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.339013477533335 5200\n",
      "2 5.102754375694117 2261\n",
      "3 1.7424639012953653 2939\n",
      "4 1.082706474824259 2261\n",
      "5 4.409507422907081 2939\n",
      "6 6.2512039712400975 2261\n",
      "7 9.54483690793526 2939\n",
      "8 8.99377302443336 2261\n",
      "9 6.952230114911771 2261\n",
      "10 4.536711086625103 2939\n",
      "11 2.206644962245922 2261\n",
      "12 0.7626290062937056 2939\n",
      "13 2.25614755760465 2261\n",
      "14 5.375156084574075 2939\n",
      "15 5.546985262636933 2261\n",
      "16 6.768573716125723 2537\n",
      "17 6.213986063382413 2261\n",
      "18 1.474639274023926 1613\n",
      "19 3.277340751504049 2939\n",
      "20 3.7568138378250318 2261\n",
      "21 6.719150773238768 2925\n",
      "22 6.0475515157692445 2261\n",
      "23 4.0800176320092625 2105\n",
      "24 2.1444537006715407 2939\n",
      "25 0.8416070412852186 2261\n",
      "26 1.6013577850836427 2939\n",
      "27 2.0779656339494075 2261\n",
      "28 4.7761526756133605 2934\n",
      "29 4.363502332903405 2261\n",
      "30 0.2056918274983488 1427\n",
      "31 3.2713164940234734 2103\n",
      "32 1.495677706804031 2939\n",
      "33 0.5396242570613481 2261\n",
      "34 1.7099495347727682 2939\n",
      "35 1.9301067780512078 2261\n",
      "36 4.366583501566485 2912\n",
      "37 3.8729286268549306 2261\n",
      "38 0.2731474026424614 1378\n",
      "39 0.6699386629508695 1584\n",
      "40 0.791626337859288 2261\n",
      "41 1.28968268725985 2939\n",
      "42 1.5027888708696437 2261\n",
      "43 3.8180439820245375 2917\n",
      "44 3.377827597677946 2261\n",
      "45 0.0400516674333069 1450\n",
      "46 2.2626640655676056 2250\n",
      "47 0.6377740118002961 2939\n",
      "48 0.10136348692333286 2261\n",
      "49 1.9346438011200675 2939\n",
      "50 1.8647053623274488 2261\n",
      "51 3.401114574019452 2712\n",
      "52 2.956034464063123 2261\n",
      "53 0.01785683813807637 1458\n",
      "54 1.7479544641471207 2257\n",
      "55 0.15905761991963985 2939\n",
      "56 0.17753449608090932 2261\n",
      "57 2.116603386842268 2933\n",
      "58 1.9138546190461625 2261\n",
      "59 2.5543824111128295 2443\n",
      "60 2.186870753175292 2261\n",
      "61 1.6206372510012912 2046\n",
      "62 1.2038104520808908 2261\n",
      "63 2.8432172259120208 2800\n",
      "64 2.434919559696901 2261\n",
      "65 0.3409292204128853 1585\n",
      "66 0.6701390961531004 2261\n",
      "67 0.9985186105664106 2938\n",
      "68 0.9834303897221872 2261\n",
      "69 2.5759723368922387 2813\n",
      "70 2.1908517558117317 2261\n",
      "71 0.49640021037254556 1653\n",
      "72 0.27078954395948074 2261\n",
      "73 1.3980440060915242 2932\n",
      "74 1.2552511524462062 2261\n",
      "75 2.2819319379634813 2615\n",
      "76 1.9286325755800249 2261\n",
      "77 0.8089777904006681 1828\n",
      "78 0.29362103560162467 2261\n",
      "79 1.92844518259189 2897\n",
      "80 1.6392943597252874 2261\n",
      "81 1.3610575453043694 2116\n",
      "82 1.0216274447402292 2261\n",
      "83 2.1121134313402266 2662\n",
      "84 1.7720528236324178 2261\n",
      "85 0.7815046129310446 1853\n",
      "86 0.31823557489614274 2261\n",
      "87 1.8493355346428935 2877\n",
      "88 1.5520500852231756 2261\n",
      "89 1.110252281676189 2044\n",
      "90 0.7653043134902653 2261\n",
      "91 1.9328975425868968 2714\n",
      "92 1.6100464013832543 2261\n",
      "93 0.8029367306364499 1895\n",
      "94 0.4000238499926212 2261\n",
      "95 1.7765483540561426 2831\n",
      "96 1.4758428301493183 2261\n",
      "97 0.9141415354527425 1977\n",
      "98 0.5639674799478058 2261\n",
      "99 1.7738478873840648 2758\n",
      "100 1.4673154329234848 2261\n",
      "101 0.7861711815421089 1925\n",
      "102 0.42099819631285484 2261\n",
      "103 1.67098153900913 2793\n",
      "104 1.377674428728696 2261\n",
      "105 0.8368241912623162 1976\n",
      "106 0.503197587633295 2261\n",
      "107 1.6262592044539907 2739\n",
      "108 1.336652140871685 2261\n",
      "109 0.7955989498720655 1972\n",
      "110 0.46810000332463275 2261\n",
      "111 1.5619862374429196 2735\n",
      "112 1.2799497083933886 2261\n",
      "113 0.7917193685853512 1988\n",
      "114 0.4784142789854826 2261\n",
      "115 1.5036704797048728 2713\n",
      "116 1.2287126035388454 2261\n",
      "117 0.7924551538486888 2012\n",
      "118 0.49299492417656054 2261\n",
      "119 1.4626626616873928 2694\n",
      "120 1.1923672377909256 2261\n",
      "121 0.7639761521475172 2013\n",
      "122 0.47140834252356 2261\n",
      "123 1.4088667716382197 2686\n",
      "124 1.1454764523167889 2261\n",
      "125 0.7615521188417098 2034\n",
      "126 0.48047778821687004 2261\n",
      "127 1.3663882896959727 2668\n",
      "128 1.1084540017004585 2261\n",
      "129 0.7459911753209454 2039\n",
      "130 0.4731574156422411 2261\n",
      "131 1.3191298777358043 2654\n",
      "132 1.0675628504574846 2261\n",
      "133 0.7658183274245189 2071\n",
      "134 0.506644453527116 2261\n",
      "135 1.265953211177461 2614\n",
      "136 1.021656119766952 2261\n",
      "137 0.7801952691884964 2098\n",
      "138 0.5323330151665928 2261\n",
      "139 1.222935745123408 2583\n",
      "140 0.9846457360084173 2261\n",
      "141 0.7773778303269054 2114\n",
      "142 0.5374904760017927 2261\n",
      "143 1.1772887324708563 2559\n",
      "144 0.9452535667569109 2261\n",
      "145 0.7772174878182554 2132\n",
      "146 0.5450301604143561 2261\n",
      "147 1.118541120208917 2526\n",
      "148 0.8939464081206339 2261\n",
      "149 0.8020304983186435 2167\n",
      "150 0.5792802860330255 2261\n",
      "151 1.0595245842985648 2477\n",
      "152 0.8420528951490596 2261\n",
      "153 0.8378566488098329 2222\n",
      "154 0.6237484750159368 2261\n",
      "155 0.9998008965695746 2423\n",
      "156 0.788659173762035 2261\n",
      "157 0.8556316152086509 2260\n",
      "158 0.6476575276549568 2261\n",
      "159 0.9413167969047507 2377\n",
      "160 0.7357275261819112 2261\n",
      "161 0.8620417297851617 2289\n",
      "162 0.6589148178929349 2261\n",
      "163 0.9033778241395609 2357\n",
      "164 0.7021849289356794 2261\n",
      "165 0.8508317939456802 2305\n",
      "166 0.6518085303376324 2261\n",
      "167 0.8719188487119851 2346\n",
      "168 0.6748199927496827 2261\n",
      "169 0.8388774476209022 2317\n",
      "170 0.6438158075053906 2261\n",
      "171 0.8471942842648161 2340\n",
      "172 0.6540123257252333 2261\n",
      "173 0.8191738713653791 2320\n",
      "174 0.6279412063873551 2261\n",
      "175 0.827680306787606 2340\n",
      "176 0.6382958898855904 2261\n",
      "177 0.8001917665487284 2320\n",
      "178 0.6127232099726925 2261\n",
      "179 0.8074488661359069 2339\n",
      "180 0.6217988746060712 2261\n",
      "181 0.7819385038095774 2321\n",
      "182 0.5981590135496191 2261\n",
      "183 0.7864793464660096 2337\n",
      "184 0.6045006557332826 2261\n",
      "185 0.7689775956735829 2324\n",
      "186 0.5888274613248186 2261\n",
      "187 0.7703996960235173 2335\n",
      "188 0.5920050327674389 2261\n",
      "189 0.7487088170349097 2323\n",
      "190 0.5721008276793356 2261\n",
      "191 0.7520766735047175 2336\n",
      "192 0.5771982698813218 2261\n",
      "193 0.736570495980807 2325\n",
      "194 0.5634524761451658 2261\n",
      "195 0.7358578524246866 2333\n",
      "196 0.5644323556473623 2261\n",
      "197 0.7176256477856242 2325\n",
      "198 0.5479140947546881 2261\n",
      "199 0.7196856372321676 2335\n",
      "200 0.5516365813144901 2261\n",
      "201 0.7034543599869494 2324\n",
      "202 0.5370967413361143 2261\n",
      "203 0.703185871996034 2333\n",
      "204 0.5384588179949787 2261\n",
      "205 0.6887001139797349 2325\n",
      "206 0.5256232024481875 2261\n",
      "207 0.6886251414860709 2333\n",
      "208 0.5271476292620747 2261\n",
      "209 0.6744369519166185 2325\n",
      "210 0.5145790403785 2261\n",
      "211 0.6745484331820535 2333\n",
      "212 0.5162571397136659 2261\n",
      "213 0.659272880791427 2324\n",
      "214 0.5025704005911251 2261\n",
      "215 0.6595770208694844 2333\n",
      "216 0.5044129161907875 2261\n",
      "217 0.6472783089715364 2326\n",
      "218 0.4936688673876759 2261\n",
      "219 0.6477348720247136 2333\n",
      "220 0.49563086398878653 2261\n",
      "221 0.6304222359036586 2322\n",
      "222 0.4798461863581167 2261\n",
      "223 0.6338383525556779 2335\n",
      "224 0.4847351688476067 2261\n",
      "225 0.6206818939405139 2325\n",
      "226 0.4730816502650441 2261\n",
      "227 0.6214680159290964 2333\n",
      "228 0.4753146422692538 2261\n",
      "229 0.6060008648263207 2323\n",
      "230 0.4613155530501487 2261\n",
      "231 0.6081032367525978 2334\n",
      "232 0.4648377676695006 2261\n",
      "233 0.5980708847388314 2327\n",
      "234 0.4562442116819767 2261\n",
      "235 0.5936469225869772 2329\n",
      "236 0.45322556658210544 2261\n",
      "237 0.5879060790900894 2328\n",
      "238 0.4488818691009797 2261\n",
      "239 0.582256687226625 2328\n",
      "240 0.44461331756631495 2261\n",
      "241 0.576694387552763 2328\n",
      "242 0.4404182468569928 2261\n",
      "243 0.5712174312053264 2328\n",
      "244 0.4362950383993436 2261\n",
      "245 0.5658241153762873 2328\n",
      "246 0.43224211858565553 2261\n",
      "247 0.5605127817928448 2328\n",
      "248 0.4282579572558731 2261\n",
      "249 0.5552818152574304 2328\n",
      "250 0.4243410662396556 2261\n",
      "251 0.5501296422448263 2328\n",
      "252 0.42177194890962966 2260\n",
      "253 0.5450208747070064 2327\n",
      "254 0.4192640568182071 2259\n",
      "255 0.5363043373999759 2323\n",
      "256 0.41564473459753515 2260\n",
      "257 0.5315531327449037 2323\n",
      "258 0.41734992913035285 2256\n",
      "259 0.5229564363926258 2320\n",
      "260 0.4098953283123947 2256\n",
      "261 0.5208413185240985 2323\n",
      "262 0.40889388238826996 2256\n",
      "263 0.5135179190882226 2321\n",
      "264 0.4026840075216356 2256\n",
      "265 0.5141084988480387 2323\n",
      "266 0.4043686579232286 2256\n",
      "267 0.5058012953472684 2320\n",
      "268 0.3984238888390763 2255\n",
      "269 0.5051643442608835 2323\n",
      "270 0.39884363586823923 2255\n",
      "271 0.4982561055522936 2320\n",
      "272 0.3943205607605407 2254\n",
      "273 0.49397160280128816 2321\n",
      "274 0.3923899886702529 2253\n",
      "275 0.4898635534363365 2320\n",
      "276 0.3892905758427929 2253\n",
      "277 0.48825530103744086 2322\n",
      "278 0.3886741876513824 2253\n",
      "279 0.4842270341361328 2320\n",
      "280 0.38563784727988804 2253\n",
      "281 0.4802590370980724 2320\n",
      "282 0.3826468607034812 2253\n",
      "283 0.4763451434684356 2320\n",
      "284 0.3809525979154884 2252\n",
      "285 0.4737299453453758 2320\n",
      "286 0.3818197367114422 2250\n",
      "287 0.469818329051499 2317\n",
      "288 0.38018340685885477 2249\n",
      "289 0.4659898777618423 2316\n",
      "290 0.3772442815471777 2249\n",
      "291 0.46353948388278354 2317\n",
      "292 0.37567021676235157 2249\n",
      "293 0.45979651350648965 2316\n",
      "294 0.3727991532091579 2249\n",
      "295 0.4574250585130315 2317\n",
      "296 0.3712870399258321 2249\n",
      "297 0.45385735736187627 2316\n",
      "298 0.3685744980004628 2249\n",
      "299 0.45156143151405104 2317\n",
      "300 0.36712105353448266 2249\n",
      "301 0.44807214225497916 2316\n",
      "302 0.36447052428874827 2249\n",
      "303 0.4446307890916472 2316\n",
      "304 0.36185735119534 2249\n",
      "305 0.44123427949116945 2316\n",
      "306 0.36051557651681065 2248\n",
      "307 0.4391116920157927 2316\n",
      "308 0.35919215725549236 2248\n",
      "309 0.4357957504189116 2315\n",
      "310 0.35666969921420844 2248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311 0.4337427203373849 2316\n",
      "312 0.35539838607522967 2248\n",
      "313 0.4304974013910723 2315\n",
      "314 0.3529311525376997 2248\n",
      "315 0.42970936490845446 2317\n",
      "316 0.35290697088093026 2248\n",
      "317 0.4241720112828631 2313\n",
      "318 0.34942369022218145 2247\n",
      "319 0.42474137469091033 2317\n",
      "320 0.35072390267562525 2247\n",
      "321 0.4168207313778254 2310\n",
      "322 0.34354952287066 2247\n",
      "323 0.41861807763817893 2318\n",
      "324 0.3460598551027658 2247\n",
      "325 0.41214146173413013 2311\n",
      "326 0.34031380708525916 2247\n",
      "327 0.4139838509433673 2318\n",
      "328 0.34285629494645065 2247\n",
      "329 0.40757678356588656 2311\n",
      "330 0.33716617441782404 2247\n",
      "331 0.40946457183130036 2318\n",
      "332 0.3397396825081742 2247\n",
      "333 0.4019447133992031 2310\n",
      "334 0.3365300712277133 2244\n",
      "335 0.4050739460946531 2316\n",
      "336 0.3402908586113469 2244\n",
      "337 0.3998653074397467 2309\n",
      "338 0.3357442176098573 2244\n",
      "339 0.39474523741770895 2309\n",
      "340 0.33493275723660787 2241\n",
      "341 0.39335209436358554 2309\n",
      "342 0.3353819093060082 2240\n",
      "343 0.39322317763966197 2309\n",
      "344 0.335824795287472 2240\n",
      "345 0.39165797578858985 2308\n",
      "346 0.3348290864556631 2240\n",
      "347 0.3888618411309963 2307\n",
      "348 0.3350649444792424 2238\n",
      "349 0.38487816549314147 2304\n",
      "350 0.33161846085339164 2238\n",
      "351 0.3858753043734211 2308\n",
      "352 0.3331337858334251 2238\n",
      "353 0.38192987763611086 2304\n",
      "354 0.330906532486388 2237\n",
      "355 0.3805191487786814 2305\n",
      "356 0.33113232715568586 2236\n",
      "357 0.3789622555871688 2304\n",
      "358 0.33006439821771494 2236\n",
      "359 0.378715793502046 2305\n",
      "360 0.33029816350735075 2236\n",
      "361 0.3771804992064176 2304\n",
      "362 0.329242769738336 2236\n",
      "363 0.37566531849111584 2304\n",
      "364 0.3292735146542043 2235\n",
      "365 0.37292730819359576 2304\n",
      "366 0.3269929364455919 2235\n",
      "367 0.37381439702069486 2305\n",
      "368 0.32833047326535336 2235\n",
      "369 0.36867480805107533 2302\n",
      "370 0.32482486499519325 2234\n",
      "371 0.37078553477082465 2305\n",
      "372 0.32618302773718 2235\n",
      "373 0.36810605103075744 2304\n",
      "374 0.3251257694460108 2234\n",
      "375 0.36663593587982035 2304\n",
      "376 0.3240796286374419 2234\n",
      "377 0.3651826928071884 2304\n",
      "378 0.32423918749359615 2233\n",
      "379 0.3636736337672204 2303\n",
      "380 0.3243209809267924 2232\n",
      "381 0.3610348903944498 2301\n",
      "382 0.323330676589066 2231\n",
      "383 0.35968365044885786 2301\n",
      "384 0.32346278882915414 2230\n",
      "385 0.3594571432119764 2301\n",
      "386 0.32359167815772594 2230\n",
      "387 0.3580120107159601 2300\n",
      "388 0.32250097197459127 2230\n",
      "389 0.3565853227529061 2300\n",
      "390 0.32142309134872704 2230\n",
      "391 0.3551746664249933 2300\n",
      "392 0.3203578687365294 2230\n",
      "393 0.35614104336873 2302\n",
      "394 0.32166188509864213 2230\n",
      "395 0.3535027432294129 2299\n",
      "396 0.31936920096702437 2230\n",
      "397 0.35327061443307883 2301\n",
      "398 0.3206285989927622 2229\n",
      "399 0.35182608539701477 2299\n",
      "400 0.31950913830889505 2229\n",
      "401 0.35040237781953465 2299\n",
      "402 0.31969938733154635 2228\n",
      "403 0.3502886724584684 2299\n",
      "404 0.3198864568035873 2228\n",
      "405 0.3501761050251275 2299\n",
      "406 0.32007176883997346 2228\n",
      "407 0.35006466687705284 2299\n",
      "408 0.3202553412701903 2228\n",
      "409 0.3487938857306929 2298\n",
      "410 0.31927882072982783 2228\n",
      "411 0.34869872758255127 2299\n",
      "412 0.3194712035408358 2228\n",
      "413 0.3452539747486796 2296\n",
      "414 0.3175045789664921 2227\n",
      "415 0.3463821723250078 2299\n",
      "416 0.3188989763270982 2227\n",
      "417 0.342033293320938 2294\n",
      "418 0.3148276112269315 2227\n",
      "419 0.3455852561265189 2301\n",
      "420 0.3186328081629455 2227\n",
      "421 0.3412426121841037 2294\n",
      "422 0.3145692373309217 2227\n",
      "423 0.3435430096363703 2300\n",
      "424 0.31711953331760867 2227\n",
      "425 0.3392297928043888 2294\n",
      "426 0.31307663724652457 2227\n",
      "427 0.34393538745953833 2302\n",
      "428 0.3180212567410621 2227\n",
      "429 0.33961324438915597 2294\n",
      "430 0.3139737560510625 2227\n",
      "431 0.3397074402740401 2298\n",
      "432 0.3143109613895092 2227\n",
      "433 0.33874413638415035 2297\n",
      "434 0.3135991567881433 2227\n",
      "435 0.3389189057887947 2298\n",
      "436 0.31401884815542047 2227\n",
      "437 0.3368799792795443 2296\n",
      "438 0.3122284560908497 2227\n",
      "439 0.3370754642154594 2298\n",
      "440 0.31266261078707913 2227\n",
      "441 0.33726539508403164 2298\n",
      "442 0.31309227280409857 2227\n",
      "443 0.3330414119932854 2294\n",
      "444 0.30911187748637303 2227\n",
      "445 0.3366981176460868 2301\n",
      "446 0.31299161836287226 2227\n",
      "447 0.33247681165389054 2294\n",
      "448 0.31012577287151544 2226\n",
      "449 0.3350754615100584 2299\n",
      "450 0.31182620695742 2227\n",
      "451 0.3319996247377759 2295\n",
      "452 0.3100953061984555 2226\n",
      "453 0.334600106438387 2299\n",
      "454 0.31290178720055906 2226\n",
      "455 0.3315155769457192 2294\n",
      "456 0.31004225040577893 2226\n",
      "457 0.33074157052516284 2296\n",
      "458 0.3094747783237679 2226\n",
      "459 0.33227009316921396 2298\n",
      "460 0.3112077967591347 2226\n",
      "461 0.32921723180926676 2294\n",
      "462 0.30837008697122315 2226\n",
      "463 0.33184264353123893 2299\n",
      "464 0.3111897793643692 2226\n",
      "465 0.327770894650655 2293\n",
      "466 0.3073331271771309 2226\n",
      "467 0.33040998866117377 2299\n",
      "468 0.31016177242329945 2226\n",
      "469 0.32747238246367516 2294\n",
      "470 0.307433863800199 2226\n",
      "471 0.32903481851174377 2298\n",
      "472 0.30918519300126945 2226\n",
      "473 0.32611612791897926 2294\n",
      "474 0.30646994756022394 2226\n",
      "475 0.32876761186295766 2299\n",
      "476 0.3093039653216749 2226\n",
      "477 0.32477060390762347 2293\n",
      "478 0.30663665453847955 2225\n",
      "479 0.32643439245695305 2297\n",
      "480 0.3073457147601246 2226\n",
      "481 0.3235496602695136 2294\n",
      "482 0.305780001680958 2225\n",
      "483 0.3252506144145073 2297\n",
      "484 0.3076494811491912 2225\n",
      "485 0.322423295151566 2293\n",
      "486 0.30500344602909024 2225\n",
      "487 0.3241363189515525 2297\n",
      "488 0.3068800564516252 2225\n",
      "489 0.32132279922692675 2293\n",
      "490 0.3042441225638086 2225\n",
      "491 0.32411893428115945 2298\n",
      "492 0.30719786168898366 2225\n",
      "493 0.32014133847149395 2292\n",
      "494 0.3033989022024386 2225\n",
      "495 0.3229490218581432 2298\n",
      "496 0.3063598071385556 2225\n",
      "497 0.3189867862576321 2292\n",
      "498 0.3025723004026304 2225\n",
      "499 0.3218040325708458 2298\n",
      "500 0.3055402029376566 2225\n",
      "Training Accuracy SVM: 0.56\n",
      "Test Accuracy SVM: 0.55\n"
     ]
    }
   ],
   "source": [
    "trainingSet = pd.read_csv(trainingDataFilename)\n",
    "testSet = pd.read_csv(testDataFilename)\n",
    "\n",
    "if modelIdx == 1:\n",
    "    w = lr(trainingSet, testSet)\n",
    "    get_accuracy_lr(w, trainingSet, testSet)\n",
    "else:\n",
    "    w = svm(trainingSet, testSet)\n",
    "    get_accuracy_svm(w, trainingSet, testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
