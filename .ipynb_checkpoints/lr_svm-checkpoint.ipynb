{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataFilename = 'trainingSet.csv'\n",
    "testDataFilename = 'testSet.csv'\n",
    "modelIdx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(scores):\n",
    "    predictions = np.zeros(len(scores))\n",
    "    for i in range(len(predictions)):\n",
    "        if scores[i] >= 0:\n",
    "            predictions[i] +=  1.0 / (1.0 + np.exp(-scores[i]))\n",
    "        else:\n",
    "            predictions[i] += np.exp(scores[i]) / (1.0 + np.exp(scores[i]))\n",
    "    return predictions\n",
    "\n",
    "def lr(trainingSet, testSet):\n",
    "    print len(trainingSet.columns)\n",
    "    regularization = 0.01\n",
    "    step_size = 0.01\n",
    "    \n",
    "    max_iterations = 500\n",
    "    tol = 1e-6\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    train_labels = trainingSet['decision']    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    \n",
    "    #print train_labels, trainingSet\n",
    "    w = np.zeros(len(trainingSet.columns) + 1)\n",
    "    \n",
    "    # Add intercept\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    #X = np.concatenate((X, intercept.T), axis=1)\n",
    "    X = np.hstack((X, intercept))\n",
    "    diff = 100.0\n",
    "    \n",
    "    while(count < max_iterations and diff > tol):\n",
    "        count += 1\n",
    "        norm_old = np.linalg.norm(w)\n",
    "        \n",
    "        scores = np.dot(X, w)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        gradient = np.dot(X.T, (predictions - Y))\n",
    "\n",
    "        for j in range(len(w)):\n",
    "            gradient[j] += regularization * w[j]\n",
    "            \n",
    "        #gradient /= len(train_labels)\n",
    "        w -= step_size * gradient\n",
    "        norm_new = np.linalg.norm(w)\n",
    "        \n",
    "        diff = abs(norm_new - norm_old)\n",
    "        #print w\n",
    "        print count, diff\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(trainingSet, testSet):\n",
    "    #print len(trainingSet.columns)\n",
    "    regularization = 0.01\n",
    "    step_size = 0.50\n",
    "    \n",
    "    max_iterations = 500\n",
    "    tol = 1e-6\n",
    "    #print len(trainingSet[trainingSet['decision'] == 1])\n",
    "    count = 0\n",
    "    train_labels = trainingSet['decision']    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "\n",
    "    w = np.zeros(len(trainingSet.columns) + 1)\n",
    "    \n",
    "    # Add intercept\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    #print train_labels\n",
    "    for i in range(len(Y)):\n",
    "        if Y[i] == 0:\n",
    "            Y[i] = -1.0\n",
    "        else:\n",
    "            Y[i] = 1.0\n",
    "    #print Y.tolist()\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((intercept, X))\n",
    "    diff = 100.0\n",
    "    while(count < max_iterations and diff > tol):\n",
    "        count += 1\n",
    "        norm_old = np.linalg.norm(w)\n",
    "        \n",
    "        predictions = np.dot(X, w)\n",
    "        #print X.shape, w.shape,predictions.shape\n",
    "#         p = []\n",
    "#         for i in range(0, 5200):\n",
    "#             p.append(sum([w[j] * X[i][j] for j in range(len(trainingSet.columns))]))\n",
    "        \n",
    "#         print predictions, p \n",
    "        #print len(predictions)\n",
    "        #print np.dot(w, X[0]), predictions[0]     \n",
    "        error = 0\n",
    "        gradient = np.zeros(len(w))\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] * Y[i] < 1.0:\n",
    "                error += 1\n",
    "                #gradient -= 1.0 * Y[i] * X[i]\n",
    "                gradient -= np.multiply(X[i], Y[i])\n",
    "            \n",
    "        gradient /= 1.0 * len(train_labels)\n",
    "        #print gradient.shape, X[0].shape\n",
    "        \n",
    "        for j in range(1, len(gradient)):\n",
    "            gradient[j] += 1.0 * regularization * w[j]\n",
    "\n",
    "        w -= 1.0 * step_size * gradient\n",
    "        norm_new = np.linalg.norm(w)\n",
    "        diff = abs(norm_new - norm_old)\n",
    "        print count, diff, error\n",
    "    print w\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_lr(w, trainingSet, testSet):\n",
    "    total_train = len(trainingSet)\n",
    "    count_train = 0\n",
    "    total_test = len(testSet)\n",
    "    count_test = 0\n",
    "    \n",
    "    train_labels = trainingSet['decision']\n",
    "    test_labels = testSet['decision']\n",
    "    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    testSet = testSet.drop('decision', axis=1)\n",
    "    \n",
    "    # Training accuracy\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((X, intercept))\n",
    "    \n",
    "    scores = np.dot(X, w)\n",
    "    predictions = sigmoid(scores)\n",
    "        \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.5:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(train_labels[i]):\n",
    "            count_train += 1\n",
    "\n",
    "    training_accuracy = 1.0 * count_train/total_train\n",
    "    print 'Training Accuracy LR:', '%.2f' % training_accuracy, count_train, total_train\n",
    "    \n",
    "    # Test accuracy\n",
    "    X = np.array(testSet)\n",
    "    Y = np.array(test_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((X, intercept))\n",
    "\n",
    "    scores = np.dot(X, w)\n",
    "    predictions = sigmoid(scores)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.5:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(test_labels[i]):\n",
    "            count_test += 1\n",
    "            \n",
    "    test_accuracy = 1.0 * count_test/total_test\n",
    "    print 'Test Accuracy LR:', '%.2f' % test_accuracy, count_test, total_test\n",
    "    \n",
    "def get_accuracy_svm(w, trainingSet, testSet):\n",
    "    total_train = len(trainingSet)\n",
    "    count_train = 0\n",
    "    total_test = len(testSet)\n",
    "    count_test = 0\n",
    "    \n",
    "    train_labels = trainingSet['decision']\n",
    "    test_labels = testSet['decision']\n",
    "    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    testSet = testSet.drop('decision', axis=1)\n",
    "    \n",
    "    # Training accuracy\n",
    "    X = np.array(trainingSet)\n",
    "    Y = np.array(train_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((intercept, X))\n",
    "    \n",
    "    predictions = np.dot(X, w)\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.0:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(Y[i]):\n",
    "            count_train += 1\n",
    "\n",
    "    training_accuracy = 1.0 * count_train/total_train\n",
    "    print 'Training Accuracy SVM:', '%.2f' % training_accuracy\n",
    "    \n",
    "    # Test accuracy\n",
    "    X = np.array(testSet)\n",
    "    Y = np.array(test_labels)\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((intercept, X))\n",
    "\n",
    "    predictions = np.dot(X, w)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.0:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    for i in range(len(predictions)):    \n",
    "        if predictions[i] == int(Y[i]):\n",
    "            count_test += 1\n",
    "            \n",
    "    test_accuracy = 1.0 * count_test/total_test\n",
    "    print 'Test Accuracy SVM:', '%.2f' % test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n",
      "1 173.62870083173343\n",
      "2 861.6362589116515\n",
      "3 514.3000810583615\n",
      "4 203.31923572662765\n",
      "5 143.94747893059412\n",
      "6 400.6045235190568\n",
      "7 747.8017883868573\n",
      "8 823.1386245055446\n",
      "9 851.5270631911828\n",
      "10 793.9097846637098\n",
      "11 517.7003425288688\n",
      "12 375.10135907427446\n",
      "13 717.807882040491\n",
      "14 724.2285001428148\n",
      "15 44.22201783018511\n",
      "16 469.55638917481815\n",
      "17 196.29164044903644\n",
      "18 13.173577571099031\n",
      "19 298.4313223228404\n",
      "20 371.1913246406658\n",
      "21 697.5184438156001\n",
      "22 633.2897973207337\n",
      "23 394.8018895610926\n",
      "24 147.74005140334202\n",
      "25 23.767432251975833\n",
      "26 254.98134342408935\n",
      "27 283.56147688399255\n",
      "28 579.3961719804336\n",
      "29 508.653253932491\n",
      "30 178.06373467519552\n",
      "31 133.03618854799197\n",
      "32 166.44983703210642\n",
      "33 443.95142198039775\n",
      "34 391.4163546368418\n",
      "35 27.89584776485117\n",
      "36 214.2035476520948\n",
      "37 6.275498911346631\n",
      "38 37.594626852847114\n",
      "39 286.13817086707786\n",
      "40 256.44120187032877\n",
      "41 364.97420707517676\n",
      "42 305.48803121336573\n",
      "43 115.56250409161999\n",
      "44 7.842955742645017\n",
      "45 240.354360767569\n",
      "46 209.23154827991902\n",
      "47 334.9555998259325\n",
      "48 274.51493170178924\n",
      "49 81.97285325977691\n",
      "50 32.517466800117745\n",
      "51 182.42234748578198\n",
      "52 154.59384970610927\n",
      "53 316.98186220395746\n",
      "54 254.1926005679834\n",
      "55 39.51301095501094\n",
      "56 98.04453539756332\n",
      "57 94.05699865605811\n",
      "58 77.57166152978289\n",
      "59 274.13087645109545\n",
      "60 213.413504763128\n",
      "61 70.64106187333664\n",
      "62 25.42679922475827\n",
      "63 170.6036431648663\n",
      "64 128.96754036834454\n",
      "65 225.50341761210302\n",
      "66 167.18753721631992\n",
      "67 128.26102062493237\n",
      "68 63.76761145162118\n",
      "69 231.22975518113708\n",
      "70 170.73237779656574\n",
      "71 82.86974057225643\n",
      "72 6.34853409130119\n",
      "73 187.6426699420599\n",
      "74 133.19162979772636\n",
      "75 139.23364998385932\n",
      "76 80.26115809992052\n",
      "77 196.29441830202904\n",
      "78 137.17284456470998\n",
      "79 98.01571279945665\n",
      "80 32.888107419735206\n",
      "81 185.3395937507098\n",
      "82 126.34366655601298\n",
      "83 93.6717399329882\n",
      "84 29.437360774822082\n",
      "85 173.87782055807247\n",
      "86 115.03427061306684\n",
      "87 93.59198129425022\n",
      "88 31.094896880454144\n",
      "89 164.3108984830751\n",
      "90 105.47179981696718\n",
      "91 90.42191490641562\n",
      "92 28.702938416655797\n",
      "93 155.32205004487423\n",
      "94 96.52298326791833\n",
      "95 88.62536349513994\n",
      "96 27.81999760370627\n",
      "97 145.7403225682765\n",
      "98 87.1734427507622\n",
      "99 89.25079138561705\n",
      "100 29.608524184438465\n",
      "101 135.66692585331703\n",
      "102 77.46357734643834\n",
      "103 91.04208122036334\n",
      "104 32.47319801894582\n",
      "105 124.95750062789512\n",
      "106 67.18814655972164\n",
      "107 92.7744703430435\n",
      "108 34.967223586840646\n",
      "109 115.37834299581027\n",
      "110 57.92258845600418\n",
      "111 93.67148552313347\n",
      "112 36.31516017670583\n",
      "113 104.56424517394407\n",
      "114 47.39938720495002\n",
      "115 94.70053745030691\n",
      "116 37.60102957223717\n",
      "117 96.65543010750889\n",
      "118 39.57643621866464\n",
      "119 91.05883183081187\n",
      "120 33.99277752316448\n",
      "121 91.30168901847173\n",
      "122 34.24504305548908\n",
      "123 86.64650716529559\n",
      "124 29.597911311681855\n",
      "125 86.7798120788384\n",
      "126 29.740231717539245\n",
      "127 82.31441509896013\n",
      "128 25.28273840494967\n",
      "129 82.37889121804074\n",
      "130 25.35758517944396\n",
      "131 78.62459191402377\n",
      "132 22.014212318533282\n",
      "133 77.73376759796611\n",
      "134 21.954368577109562\n",
      "135 74.64512095723421\n",
      "136 19.14663534398096\n",
      "137 73.38098121384792\n",
      "138 18.029308753401892\n",
      "139 71.2160262037296\n",
      "140 16.410746958220443\n",
      "141 69.35394467929746\n",
      "142 14.686023094984193\n",
      "143 67.70664291090543\n",
      "144 13.181266485149536\n",
      "145 66.06640103004156\n",
      "146 11.926587798803666\n",
      "147 64.55311667662409\n",
      "148 11.18294709611564\n",
      "149 62.47652965661382\n",
      "150 9.737242508072995\n",
      "151 61.055747787072505\n",
      "152 8.811678597377067\n",
      "153 59.58541472877823\n",
      "154 7.60447925103108\n",
      "155 58.209667498947965\n",
      "156 6.812525585409276\n",
      "157 56.93969556705997\n",
      "158 6.118231462284712\n",
      "159 55.66258214622667\n",
      "160 5.778517221484435\n",
      "161 54.07038006082621\n",
      "162 4.6662208766983895\n",
      "163 52.87160508849229\n",
      "164 3.930220806827492\n",
      "165 52.10272522467858\n",
      "166 3.6203254956781166\n",
      "167 50.513482006173945\n",
      "168 2.5640611666012774\n",
      "169 49.3604935576559\n",
      "170 1.8757577693049825\n",
      "171 48.357393998113366\n",
      "172 1.4173590947511912\n",
      "173 47.358744236887105\n",
      "174 0.7501053213327395\n",
      "175 45.90657529866803\n",
      "176 0.07880509407186764\n",
      "177 44.84516817131589\n",
      "178 0.461937029514047\n",
      "179 43.38726092671004\n",
      "180 1.3792568757971821\n",
      "181 42.56972430710084\n",
      "182 1.7759610684452127\n",
      "183 41.18829711698072\n",
      "184 2.654599762758153\n",
      "185 40.32287112047743\n",
      "186 2.505755108134508\n",
      "187 38.59081801238972\n",
      "188 3.21825048343635\n",
      "189 37.88607918518392\n",
      "190 3.5964130453576217\n",
      "191 36.629883063847956\n",
      "192 4.257433212319484\n",
      "193 35.97719405477619\n",
      "194 4.609275639453699\n",
      "195 34.881973195394494\n",
      "196 4.837715967499207\n",
      "197 33.7084457452911\n",
      "198 5.031234711049365\n",
      "199 33.32416497394115\n",
      "200 5.220223161281865\n",
      "201 32.362980142324886\n",
      "202 6.091289055539164\n",
      "203 31.725981190789753\n",
      "204 5.7448140408287145\n",
      "205 31.11183469063144\n",
      "206 5.961374773631178\n",
      "207 30.352519110790126\n",
      "208 6.26127982048456\n",
      "209 29.608445780629154\n",
      "210 6.17590306026068\n",
      "211 28.553447780135684\n",
      "212 6.570937897333351\n",
      "213 27.87509935459184\n",
      "214 6.621623379769517\n",
      "215 27.303132705778808\n",
      "216 6.60210019436181\n",
      "217 27.158098080903983\n",
      "218 6.409739060457468\n",
      "219 25.67476187979355\n",
      "220 6.13856348545869\n",
      "221 24.61171262956941\n",
      "222 6.4811405995069435\n",
      "223 24.33009142262017\n",
      "224 6.249971296762851\n",
      "225 23.252272027490108\n",
      "226 6.633189684423087\n",
      "227 22.88024512840093\n",
      "228 6.774524579542231\n",
      "229 22.156563603647555\n",
      "230 6.191858910880001\n",
      "231 21.050775562094714\n",
      "232 6.480528478527049\n",
      "233 20.481750506121898\n",
      "234 6.419667087385278\n",
      "235 20.274600031138107\n",
      "236 6.148002724066828\n",
      "237 19.415691351639907\n",
      "238 6.2590394060653125\n",
      "239 19.154351653171943\n",
      "240 6.360541165747236\n",
      "241 18.919867794932543\n",
      "242 6.362221714039151\n",
      "243 18.494739099891376\n",
      "244 6.402746862620916\n",
      "245 17.91209911340775\n",
      "246 6.227508401426348\n",
      "247 17.275289934371358\n",
      "248 6.234436044871472\n",
      "249 16.665958852410768\n",
      "250 5.994877823426577\n",
      "251 16.325316691327316\n",
      "252 6.043466259666275\n",
      "253 15.993944099815963\n",
      "254 5.924137894109663\n",
      "255 15.424885091820215\n",
      "256 5.733009055665207\n",
      "257 15.242882776352417\n",
      "258 5.55037340385843\n",
      "259 14.922020303568388\n",
      "260 5.366338348923819\n",
      "261 14.745117045101324\n",
      "262 5.029458320730555\n",
      "263 13.921383330594836\n",
      "264 4.9592215969423705\n",
      "265 13.931549791641373\n",
      "266 4.963259101550648\n",
      "267 13.7189316003969\n",
      "268 4.881737956018696\n",
      "269 13.454835408710096\n",
      "270 4.613331832606491\n",
      "271 13.232289312949433\n",
      "272 4.56544402540294\n",
      "273 12.924161827035277\n",
      "274 4.247434087076726\n",
      "275 12.477787540999998\n",
      "276 4.499158609812184\n",
      "277 12.527113748017655\n",
      "278 4.418881798944312\n",
      "279 12.107022592479552\n",
      "280 4.549487832485283\n",
      "281 12.059333329038054\n",
      "282 4.531308141401496\n",
      "283 11.944921059599437\n",
      "284 4.444363131185128\n",
      "285 11.799847580572532\n",
      "286 4.393347835654822\n",
      "287 11.798908148734881\n",
      "288 4.207023447986103\n",
      "289 11.660254112960502\n",
      "290 4.265338180013714\n",
      "291 11.617555344690118\n",
      "292 4.266060295773968\n",
      "293 11.22177064238349\n",
      "294 3.8614036611370466\n",
      "295 10.876502178834016\n",
      "296 4.0257772408322126\n",
      "297 10.803517635675234\n",
      "298 3.8307655080761833\n",
      "299 10.603537202769076\n",
      "300 3.9081094482153276\n",
      "301 10.59404307337536\n",
      "302 3.933535626054436\n",
      "303 10.338161190476058\n",
      "304 3.9332390063382263\n",
      "305 10.306778841728374\n",
      "306 3.8984084276244175\n",
      "307 10.180074676580261\n",
      "308 3.8384036374473\n",
      "309 10.069847303955612\n",
      "310 3.7539094030016713\n",
      "311 9.975162644070224\n",
      "312 3.729448457715989\n",
      "313 9.944791974595319\n",
      "314 3.709730253739508\n",
      "315 9.840989964965956\n",
      "316 3.761041608632695\n",
      "317 9.805782789689147\n",
      "318 3.8120785450109906\n",
      "319 9.708263275495483\n",
      "320 3.9252570520684458\n",
      "321 9.67342059548173\n",
      "322 3.7712591122317463\n",
      "323 9.51211388487809\n",
      "324 3.667466026895454\n",
      "325 9.497558420104724\n",
      "326 3.695195463134951\n",
      "327 9.426125485854755\n",
      "328 3.7105776410735416\n",
      "329 9.283499166238471\n",
      "330 3.7475105394014463\n",
      "331 9.262455072320336\n",
      "332 3.459072087781351\n",
      "333 9.12183747094241\n",
      "334 3.403909598678183\n",
      "335 8.799760365369366\n",
      "336 3.4214676579740626\n",
      "337 8.675759990578626\n",
      "338 3.382555857272564\n",
      "339 8.596868289700978\n",
      "340 3.2559897585742874\n",
      "341 8.52966520764221\n",
      "342 3.2848346370228683\n",
      "343 8.457166913638503\n",
      "344 3.124419837948153\n",
      "345 8.199838260376055\n",
      "346 3.2591434528903847\n",
      "347 8.376765151407199\n",
      "348 3.162924989982457\n",
      "349 8.070214606916124\n",
      "350 3.0935034765234377\n",
      "351 7.910765813569924\n",
      "352 3.0111616264075565\n",
      "353 7.804414145978626\n",
      "354 3.070132478414962\n",
      "355 7.714441929758323\n",
      "356 2.9226201309738826\n",
      "357 7.693833820265354\n",
      "358 2.9042280768580895\n",
      "359 7.670395316507893\n",
      "360 2.814283753881682\n",
      "361 7.717449795585708\n",
      "362 2.850390617263656\n",
      "363 7.697381946282803\n",
      "364 2.88877849857181\n",
      "365 7.622705514822883\n",
      "366 2.717414290772467\n",
      "367 7.348009447218828\n",
      "368 2.7677412507446206\n",
      "369 7.381073642699448\n",
      "370 2.7423059002830996\n",
      "371 7.421548926328796\n",
      "372 2.715992745438598\n",
      "373 7.2275702883443955\n",
      "374 2.7397472162901977\n",
      "375 7.251203892775266\n",
      "376 2.7309806604989717\n",
      "377 7.111932619522122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378 2.7638991666171933\n",
      "379 7.138066353828435\n",
      "380 2.7524188763700295\n",
      "381 7.097540399291574\n",
      "382 2.748147964985037\n",
      "383 7.163172914810275\n",
      "384 2.696990681495663\n",
      "385 6.9500325957142195\n",
      "386 2.8646668580231562\n",
      "387 6.962718646667781\n",
      "388 2.806582785552564\n",
      "389 6.9246811606562915\n",
      "390 2.7472611887751555\n",
      "391 6.807207169562389\n",
      "392 2.570521403105886\n",
      "393 6.703232563576421\n",
      "394 2.570384091682172\n",
      "395 6.554744817620303\n",
      "396 2.613960665040395\n",
      "397 6.744924934165283\n",
      "398 2.556749844126898\n",
      "399 6.53785759051425\n",
      "400 2.598156933120663\n",
      "401 6.579822677192169\n",
      "402 2.631762003201402\n",
      "403 6.663055523360526\n",
      "404 2.5626665437348493\n",
      "405 6.457288009297372\n",
      "406 2.5409058739442116\n",
      "407 6.320870624067538\n",
      "408 2.5679148517911017\n",
      "409 6.331494375098373\n",
      "410 2.5714016397760133\n",
      "411 6.269826506400932\n",
      "412 2.524927365446274\n",
      "413 6.199381812904903\n",
      "414 2.4954244829104937\n",
      "415 6.186484083332289\n",
      "416 2.4736100417876514\n",
      "417 6.08196974722614\n",
      "418 2.408768986681025\n",
      "419 6.007811280650458\n",
      "420 2.496938058411615\n",
      "421 6.039474895105741\n",
      "422 2.4792221349816828\n",
      "423 6.01295437492081\n",
      "424 2.5196361399321177\n",
      "425 6.065613960749033\n",
      "426 2.5396554993385507\n",
      "427 6.200049787717944\n",
      "428 2.4771901405729295\n",
      "429 5.803345464403719\n",
      "430 2.2308588155510733\n",
      "431 5.633449479615592\n",
      "432 2.298495577451831\n",
      "433 5.580047098299474\n",
      "434 2.306423069696393\n",
      "435 5.529178312013755\n",
      "436 2.3712497440374136\n",
      "437 5.533533345467731\n",
      "438 2.380745393756115\n",
      "439 5.648602017336998\n",
      "440 2.337471419415124\n",
      "441 5.641766752070907\n",
      "442 2.3576347035086656\n",
      "443 5.620948367859455\n",
      "444 2.3651690906426666\n",
      "445 5.568997409418444\n",
      "446 2.398847930883676\n",
      "447 5.601024062364559\n",
      "448 2.379574843098453\n",
      "449 5.617064650505199\n",
      "450 2.376225328217515\n",
      "451 5.532485741380697\n",
      "452 2.4145103299970287\n",
      "453 5.675901088558021\n",
      "454 2.3924151090568557\n",
      "455 5.659075630414918\n",
      "456 2.417606197931491\n",
      "457 5.635696407211981\n",
      "458 2.40820076631735\n",
      "459 5.491413082111649\n",
      "460 2.2192331808346353\n",
      "461 5.087568911889321\n",
      "462 2.299655743132462\n",
      "463 5.281648564521674\n",
      "464 2.329292168835309\n",
      "465 5.34785196874418\n",
      "466 2.221713039849419\n",
      "467 5.328741062885456\n",
      "468 2.252798353572871\n",
      "469 5.4008115517581246\n",
      "470 2.192648828534402\n",
      "471 5.360633762118596\n",
      "472 2.191773685611224\n",
      "473 5.3734111303037935\n",
      "474 2.1883837115392453\n",
      "475 5.388626377643959\n",
      "476 2.232009741392176\n",
      "477 5.576140313381984\n",
      "478 2.3908426056768803\n",
      "479 5.657116650068929\n",
      "480 2.4611238067027443\n",
      "481 5.738402744349514\n",
      "482 2.4354991858181165\n",
      "483 5.627790845879645\n",
      "484 2.3887917189331347\n",
      "485 5.572731256161205\n",
      "486 2.3480160398994485\n",
      "487 5.571347279745169\n",
      "488 2.279384357500021\n",
      "489 5.344608066103319\n",
      "490 2.206818072265378\n",
      "491 5.160858345050656\n",
      "492 2.053991109285562\n",
      "493 4.886906747380635\n",
      "494 1.8719399433120998\n",
      "495 4.431713230880632\n",
      "496 1.7309167038565647\n",
      "497 4.40506121690305\n",
      "498 1.7134041917615832\n",
      "499 4.463260882410395\n",
      "500 1.6670063868004945\n",
      "Training Accuracy LR: 0.65 3400 5200\n",
      "Test Accuracy LR: 0.65 843 1300\n"
     ]
    }
   ],
   "source": [
    "trainingSet = pd.read_csv(trainingDataFilename)\n",
    "testSet = pd.read_csv(testDataFilename)\n",
    "\n",
    "if modelIdx == 1:\n",
    "    w = lr(trainingSet, testSet)\n",
    "    get_accuracy_lr(w, trainingSet, testSet)\n",
    "else:\n",
    "    w = svm(trainingSet, testSet)\n",
    "    get_accuracy_svm(w, trainingSet, testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
